{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyPyFgjq0x44BaTzUAKLpIvi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"w8_a9BLQTIJF","executionInfo":{"status":"ok","timestamp":1702674438996,"user_tz":300,"elapsed":6553,"user":{"displayName":"Andy He","userId":"10835203185901519708"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision\n","import torch.nn.init as init\n","import tqdm\n","import sys\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","REAL = 1.\n","FAKE = 0."]},{"cell_type":"code","source":["class Reshape(nn.Module):\n","    def __init__(self, shape):\n","        super(Reshape, self).__init__()\n","        self.shape = shape\n","\n","    def forward(self, x):\n","        return x.view(*self.shape)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features=110, out_features=384*4*4),\n","            nn.ReLU(inplace=True),\n","\n","            Reshape((-1, 384, 4, 4)),\n","\n","            nn.ConvTranspose2d(in_channels=384, out_channels=192, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(in_channels=192, out_channels=96, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.BatchNorm2d(96),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(in_channels=96, out_channels=3, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.Tanh()\n","        )\n","        self.apply(self.init_weights)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input\n","        return self.model(x)\n","\n","    def init_weights(self, m):\n","      if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n","        init.normal_(m.weight, mean=0, std=0.02)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.relu1 = nn.LeakyReLU(0.2)\n","        self.dropout1 = nn.Dropout(0.5)\n","\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.batch_norm2 = nn.BatchNorm2d(32)\n","        self.relu2 = nn.LeakyReLU(0.2)\n","        self.dropout2 = nn.Dropout(0.5)\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.batch_norm3 = nn.BatchNorm2d(64)\n","        self.relu3 = nn.LeakyReLU(0.2)\n","        self.dropout3 = nn.Dropout(0.5)\n","\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.batch_norm4 = nn.BatchNorm2d(128)\n","        self.relu4 = nn.LeakyReLU(0.2)\n","        self.dropout4 = nn.Dropout(0.5)\n","\n","        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.batch_norm5 = nn.BatchNorm2d(256)\n","        self.relu5 = nn.LeakyReLU(0.2)\n","        self.dropout5 = nn.Dropout(0.5)\n","\n","        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.batch_norm6 = nn.BatchNorm2d(512)\n","        self.relu6 = nn.LeakyReLU(0.2)\n","        self.dropout6 = nn.Dropout(0.5)\n","\n","        self.discriminator = nn.Linear(512 * 4 * 4, 1)\n","        self.classifier = nn.Linear(512 * 4 * 4, 10)\n","\n","        self.softmax = nn.Softmax(dim=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.apply(self.init_weights)\n","\n","\n","    def forward(self, x):\n","        x = self.dropout1(self.relu1(self.conv1(x)))\n","        x = self.dropout2(self.relu2(self.batch_norm2(self.conv2(x))))\n","        x = self.dropout3(self.relu3(self.batch_norm3(self.conv3(x))))\n","        x = self.dropout4(self.relu4(self.batch_norm4(self.conv4(x))))\n","        x = self.dropout5(self.relu5(self.batch_norm5(self.conv5(x))))\n","        x = self.dropout6(self.relu6(self.batch_norm6(self.conv6(x))))\n","        x = x.view(-1, 512 * 4 * 4)\n","        discriminator = self.discriminator(x)\n","        classifier = self.classifier(x)\n","        choice = self.sigmoid(discriminator).view(-1, 1).squeeze(1)\n","        classes = self.softmax(classifier)\n","\n","        return choice, classes\n","    def init_weights(self, m):\n","      if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        init.normal_(m.weight, mean=0, std=0.02)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)"],"metadata":{"id":"2YPsHKeNTNg8","executionInfo":{"status":"ok","timestamp":1702674438996,"user_tz":300,"elapsed":3,"user":{"displayName":"Andy He","userId":"10835203185901519708"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def scale_transform(image):\n","  return image * 2 - 1\n","def get_data() -> DataLoader:\n","  data_transforms = transforms.Compose([\n","    transforms.Resize(32),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    scale_transform\n","  ])\n","\n","  dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=data_transforms)\n","  # loader = DataLoader(dataset, batch_size=100, shuffle=False)\n","  loader = DataLoader(dataset, batch_size=100, shuffle=True)\n","\n","  return loader\n","\n","def create_random_seeding():\n","  batch_tensors = []\n","  labels = []\n","  for _ in range(100):\n","      random_part = torch.normal(mean=0, std = 1, size = (100, 1, 1)) # better to use uniform random or normal random?\n","      one_hot_vector = torch.zeros(10, 1, 1)\n","      label = torch.randint(0, 10, (1,))\n","      labels.append(label)\n","      one_hot_vector[label] = 1\n","      complete_tensor = torch.cat((random_part, one_hot_vector), dim=0)\n","      batch_tensors.append(complete_tensor)\n","\n","  final_tensor = torch.stack(batch_tensors)\n","\n","  return final_tensor.clone().detach().requires_grad_(True), torch.tensor(labels)\n","\n","def get_dis_accuracies(dis_labels, dis_predictions):\n","  correct = 0\n","\n","  for y, y_hat in zip(dis_labels, dis_predictions):\n","    if y == round(float(y_hat.detach().cpu().numpy())):\n","      correct += 1\n","  return correct / len(dis_labels)\n","\n","def get_class_accuracies(class_labels, class_predictions):\n","  correct = 0\n","  for y, y_hat in zip(class_labels, class_predictions):\n","    if y == y_hat.argmax(dim = -1):\n","      correct += 1\n","  return correct / len(class_labels)\n","def save_model_state(model, epoch, filepath):\n","  state = {\n","      'state_dict': model.state_dict(),\n","      'epoch': epoch\n","  }\n","  torch.save(state, filepath)\n","\n","def create_sample_seeding() -> torch.Tensor:\n","  batch_tensors = []\n","  for i in range(20):\n","      random_part = torch.normal(mean=0, std = 1, size = (100, 1, 1)) # better to use uniform random or normal random?\n","      one_hot_vector = torch.zeros(10, 1, 1)\n","      label = i % 10\n","      one_hot_vector[label] = 1\n","      complete_tensor = torch.cat((random_part, one_hot_vector), dim=0)\n","      batch_tensors.append(complete_tensor)\n","\n","  final_tensor = torch.stack(batch_tensors)\n","\n","  return final_tensor\n","\n","def sample_images(netG, save=False, epoch=0):\n","  seeds = create_sample_seeding()\n","  seeds = seeds.to(device)\n","  netG.eval()\n","  with torch.no_grad():\n","    imgs = netG(seeds)\n","\n","  imgs = imgs / 2 + .5\n","  grid = torchvision.utils.make_grid(imgs, nrow=10, padding = 4)\n","  grid = grid.detach().cpu().numpy()\n","  plt.imshow(np.transpose(grid, (1,2,0)))\n","  plt.xticks([])\n","  plt.yticks([])\n","  if save:\n","    plt.savefig(f\"img_e{epoch}.png\")\n","    np.save(f\"grid_e{epoch}.npy\", imgs.detach().cpu().numpy())\n","  else:\n","    plt.show()\n","\n","def sample_real_images():\n","  dataloader = get_data()\n","  data_iter = iter(dataloader)\n","  images, labels = next(data_iter)\n","  sample_image, sample_label = images[0], labels[0]\n","  sample_image = sample_image / 2 + .5\n","  plt.imshow(np.transpose(sample_image, (1,2,0)))\n","  plt.show()"],"metadata":{"id":"THQY8nNBTb5E","executionInfo":{"status":"ok","timestamp":1702674438996,"user_tz":300,"elapsed":2,"user":{"displayName":"Andy He","userId":"10835203185901519708"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["netG = Generator()\n","netD = Discriminator()\n","\n","G_path = \"G_100wac.pth\"\n","D_path = \"D_100wac.pth\"\n","netG.load_state_dict(torch.load(G_path)['state_dict'])\n","netD.load_state_dict(torch.load(D_path)['state_dict'])\n","\n","netG = netG.to(device)\n","netD = netD.to(device)\n","\n","netG.train()\n","netD.train()\n","\n","dis_criterion = nn.BCELoss().to(device)\n","class_criterion = nn.NLLLoss().to(device)\n","\n","optimizer_G = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5, 0.999))\n","optimizer_D = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\n","\n","dataloader = get_data()\n","\n","gradientsD = []\n","gradientsG = []\n","\n","for real_Xs, real_ys in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n","  # for real_Xs, real_ys in dataloader:\n","    # TRAIN DISCRIMINATOR\n","    netD.train()\n","    netD.zero_grad()\n","\n","    real_Xs = real_Xs.to(device)\n","    real_ys = real_ys.to(device)\n","\n","    # D on real data\n","    dis_prediction, class_prediction = netD(real_Xs)\n","    dis_labels = torch.full((100, ), REAL, requires_grad=True).to(device)\n","    loss_R = dis_criterion(dis_prediction, dis_labels) + class_criterion(class_prediction, real_ys)\n","    loss_R.backward()\n","\n","    # Generate fake data\n","    fake_seeding, fake_labels = create_random_seeding()\n","    fake_seeding = fake_seeding.to(device)\n","    fake_labels = fake_labels.to(device)\n","    fake_Xs = netG(fake_seeding)\n","    fake_Xs = fake_Xs.to(device)\n","\n","    # D on fake data\n","    dis_prediction, class_prediction = netD(fake_Xs.detach())\n","    dis_labels = torch.full((100, ), FAKE, requires_grad=True).to(device)\n","    loss_F = class_criterion(class_prediction, fake_labels) + dis_criterion(dis_prediction, dis_labels)\n","    loss_F.backward()\n","\n","    gradientsD = [param.grad for param in netD.parameters()]\n","    netD.zero_grad()\n","\n","    # TRAIN GENERATOR\n","    netG.train()\n","    netG.zero_grad()\n","\n","    dis_prediction, class_prediction = netD(fake_Xs)\n","    dis_labels = torch.full((100, ), REAL, requires_grad=True).to(device)\n","\n","    loss_G = class_criterion(class_prediction, fake_labels) + dis_criterion(dis_prediction, dis_labels)\n","    loss_G.backward()\n","    gradientsG = [param.grad for param in netG.parameters()]\n","    optimizer_G.step()\n","\n","    break\n","print(gradientsG)\n","print(gradientsD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"kkeobs17TPEH","executionInfo":{"status":"error","timestamp":1702674826289,"user_tz":300,"elapsed":391,"user":{"displayName":"Andy He","userId":"10835203185901519708"}},"outputId":"db1e8b0a-38a2-407b-e1a8-c4432e442ce4"},"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-58afb45d4bca>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mG_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"G_100wac.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mD_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"D_100wac.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'G_100wac.pth'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AiYExxoIVAHZ"},"execution_count":null,"outputs":[]}]}